{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from recsys4daos.models import OpenPop\n",
    "from recsys4daos.datasets import to_microsoft\n",
    "from recsys4daos.model_selection import cvtt_open\n",
    "from recsys4daos.evaluation import calculate_all_metrics\n",
    "\n",
    "import paths\n",
    "\n",
    "import recommenders\n",
    "if recommenders.__version__ == '1.2.0':\n",
    "    print(\"Ignoring warnings\")\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "SPLITS_FREQ = \"W-THU\"\n",
    "SPLITS_NORMALIZE = True\n",
    "ORG_NAME = 'Decentraland'\n",
    "\n",
    "# In this notebook this is just used for plotting and description, not \"training\"\n",
    "# every fold is saved into the results table\n",
    "LAST_FOLDS = 10\n",
    "\n",
    "K_RECOMMENDATIONS: List[int] = [1,3,5,10,15,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!md5sum ../data/decentraland/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = paths.load_proposals(ORG_NAME)\n",
    "dfv = paths.load_votes(ORG_NAME)\n",
    "\n",
    "df = to_microsoft(dfv)\n",
    "\n",
    "print(dfp.info())\n",
    "print(dfv.info())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing some info of the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdd = defaultdict(list)\n",
    "\n",
    "for dftrain, dftest, t, openproposals in cvtt_open(to_microsoft(dfv), SPLITS_FREQ, dfp.reset_index(), remove_not_in_train_col='userID', normalize=SPLITS_NORMALIZE):\n",
    "    dftrain_filter = dftrain[dftrain['itemID'].isin(openproposals)]\n",
    "    \n",
    "    sdd['t'].append(t)\n",
    "    sdd['open proposals'].append(len(openproposals))\n",
    "    \n",
    "    sdd['proposals in train'].append(dftrain['itemID'].nunique())\n",
    "    sdd['votes in train'].append(len(dftrain))\n",
    "    sdd['votes in open proposals (train)'].append(len(dftrain_filter))\n",
    "    sdd['users in open proposals (train)'].append(dftrain_filter['userID'].nunique())\n",
    "    sdd['votes in test'].append(len(dftest))\n",
    "    sdd['users in train'].append(dftrain['userID'].nunique())\n",
    "    sdd['users in test'].append(dftest['userID'].nunique())\n",
    "\n",
    "sdf_all = pd.DataFrame(sdd).set_index('t')\n",
    "sdf_all['vpp in open proposals (train)'] = sdf_all['votes in open proposals (train)'] / sdf_all['open proposals']\n",
    "sdf_all['vpu in open proposals (train)'] = sdf_all['votes in open proposals (train)'] / sdf_all['users in open proposals (train)']\n",
    "sdf_all['vpp test'] = sdf_all['votes in test'] / sdf_all['open proposals']\n",
    "sdf_all['vpu test'] = sdf_all['votes in test'] / sdf_all['users in test']\n",
    "sdf = sdf_all.tail(LAST_FOLDS)\n",
    "print(sdf['votes in train'])\n",
    "_style = sdf[['open proposals', 'votes in open proposals (train)', 'users in open proposals (train)', 'vpp in open proposals (train)', 'vpu in open proposals (train)', 'votes in test', 'users in test', 'vpp test', 'vpu test']].style\n",
    "_style = _style.format_index('{:%G-W%V}').format(precision=2)\n",
    "_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_style\n",
    "  .format_index(\"\\\\textbf{{{}}}\", escape=\"latex\", axis=1)\n",
    "  .to_latex()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running openpop baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = list(cvtt_open(df, SPLITS_FREQ, dfp, remove_not_in_train_col='userID', normalize=SPLITS_NORMALIZE))\n",
    "metrics = []\n",
    "perfectmetrics = []\n",
    "\n",
    "for i, (train, test, t, open_proposals) in enumerate(tqdm(folds)):\n",
    "    assert not train.empty, f\"Train should not be empty on fold {i}\"\n",
    "    if test.empty:\n",
    "        print(f\"Warning, empty test fold {i}\", file=sys.stderr)\n",
    "\n",
    "    mdict = {'t': t}\n",
    "    pdict = {'t': t}\n",
    "    model = OpenPop(train)\n",
    "    recs = model.recommend_k_items(test['userID'].unique(), max(K_RECOMMENDATIONS), recommend_from=open_proposals)\n",
    "    mdict |= calculate_all_metrics(test, recs, K_RECOMMENDATIONS)\n",
    "        # for m, f in metrics_f.items():\n",
    "        #     r = f(test, recs, k=k_recs)\n",
    "        #     metrics[m+f'@{k_recs}'].append(r)\n",
    "    \n",
    "    recs = test.copy()\n",
    "    recs['prediction'] = 1\n",
    "    pdict |= calculate_all_metrics(test, recs, K_RECOMMENDATIONS)\n",
    "\n",
    "        # for m, f in metrics_f.items():\n",
    "        #     # Need relevancy_method=None as they are unsorted\n",
    "        #     r = f(test, recs, k=k_recs)\n",
    "        #     perfectmetrics[m+f'@{k_recs}'].append(r)\n",
    "\n",
    "    metrics.append(mdict)\n",
    "    perfectmetrics.append(pdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf = pd.DataFrame(metrics).set_index(\"t\")\n",
    "paths.save_openpop(odf, ORG_NAME, SPLITS_FREQ, SPLITS_NORMALIZE)\n",
    "odf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(perfectmetrics).set_index(\"t\")\n",
    "paths.save_perfect(pdf, ORG_NAME, SPLITS_FREQ, SPLITS_NORMALIZE)\n",
    "display(pdf.describe())\n",
    "assert all( (0 <= pdf.min()) & (pdf.max() <= 1) ), \"There are metrics with wrong range\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting some graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = pd.DataFrame(metrics)\n",
    "mdf['perfect precision@5'] = pdf['precision@5']\n",
    "mdf['perfect precision@10'] = pdf['precision@10']\n",
    "mdf[['precision@5', 'perfect precision@5', 'precision@10', 'perfect precision@10']].plot(title='EvaluaciÃ³n modelo baseline MP')\n",
    "mdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf[-LAST_FOLDS:].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
