{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray import train, tune\n",
    "\n",
    "from recommenders.evaluation.python_evaluation import metrics as metrics_dict\n",
    "\n",
    "from recsys24_daos.model_selection import cvtt_open\n",
    "from recsys24_daos.datasets import to_microsoft, filter_window_size\n",
    "from recsys24_daos.models import LightGCNCustom\n",
    "import recsys24_daos.utils.notebooks as nbu\n",
    "\n",
    "import recommenders\n",
    "if recommenders.__version__ == '1.2.0':\n",
    "    print(\"Ignoring warnings\")\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import paths\n",
    "\n",
    "nbu.print_versions('ray', 'tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Others config\n",
    "SEED: int = 57\n",
    "RAY_RESULTS_PATH: Path = '~/ray_results3.11'\n",
    "\n",
    "# Dataset splits config\n",
    "ORG_NAME = 'Decentraland'\n",
    "SPLITS_FREQ = 'W-THU'  # Split weekly\n",
    "LAST_SPLITS = 10  # Use just last 10 splits\n",
    "SPLITS_NORMALIZE = True\n",
    "\n",
    "# Training config\n",
    "SMALL_EXPERIMENT: bool = os.uname().nodename != 'lamarck'\n",
    "MAX_EPOCHS: int = 200\n",
    "EPOCHS_PER_ITER: int = 5\n",
    "SAMPLES_PER_SPLIT: int = 100\n",
    "OPTIM_METRIC: str = 'map@10'\n",
    "\n",
    "# Search space config\n",
    "MAX_EMBEDDING_DIM = 1024\n",
    "MAX_BATCH_SIZE = 10 # 2**10\n",
    "MIN_LR = 1e-4\n",
    "WINDOW_SIZES = ['7d', '14d', '21d', '30d', '60d', '90d', '10Y']\n",
    "GPUS = 16\n",
    "\n",
    "# Eval config\n",
    "K_RECOMMENDATIONS: List[int] = [1,3,5,10,15,100]\n",
    "METRICS: List[str] = [\"recall\", \"ndcg\", \"precision\", \"map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAY_RESULTS_PATH = Path(RAY_RESULTS_PATH).expanduser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = paths.load_proposals(ORG_NAME)\n",
    "dfv = paths.load_votes(ORG_NAME)\n",
    "\n",
    "print(dfp.info())\n",
    "print(dfv.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = to_microsoft(dfv)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = list(cvtt_open(df, SPLITS_FREQ, dfp, remove_not_in_train_col='userID'))\n",
    "print(len(folds), \"folds\")\n",
    "folds = folds[-LAST_SPLITS:]\n",
    "for i, (dftrain, dftest, t, open_proposals) in enumerate(folds):\n",
    "    min_train = dftrain['timestamp'].min().date()\n",
    "    max_train = dftrain['timestamp'].max().date()\n",
    "    min_test  = dftest['timestamp'].min().date()\n",
    "    max_test  = dftest['timestamp'].max().date()\n",
    "\n",
    "    train_users = len(set(dftrain['userID']))\n",
    "    test_users = len(set(dftest['userID']))\n",
    "    \n",
    "    print(f\"Split {i}, train from: {min_train} to {max_train}, test from: {min_test} to {max_test}\")\n",
    "    print(f\"  t: {t}\")\n",
    "    print(f\"  open proposals: {len(open_proposals)}\")\n",
    "    print(f\"  len(train): {len(dftrain)}, len(test): {len(dftest)}\")\n",
    "    print(f\"  users(train): {train_users}, users(test): {test_users}\")\n",
    "\n",
    "    print()\n",
    "    dftest['prediction'] = 1\n",
    "    for m in METRICS:\n",
    "        f = metrics_dict[f'{m}_at_k']\n",
    "        print(f\"  highest possible {m}@{K_RECOMMENDATIONS[0]}:\\t{f(dftest, dftest, k=K_RECOMMENDATIONS[0], relevancy_method='top_k'):.4f}\")\n",
    "\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.models.deeprec.DataModel.ImplicitCF import ImplicitCF\n",
    "from recommenders.models.deeprec.deeprec_utils import prepare_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(\n",
    "    model_type='lightgcn',\n",
    "    n_layers=3,\n",
    "    batch_size=512,\n",
    "    embed_size=64,\n",
    "    epochs=2,\n",
    "    learning_rate=0.001,\n",
    "    decay=0.001,\n",
    "    metrics=[\"recall\", \"ndcg\", \"precision\", \"map\"],\n",
    "    eval_epoch=2,\n",
    "    top_k=K_RECOMMENDATIONS[0],\n",
    "    save_model=False,\n",
    "    MODEL_DIR='./data/model/lightgcn/',\n",
    ")\n",
    "dataloader = ImplicitCF(train=folds[-1][0], test=folds[-1][1], seed=SEED)\n",
    "print(\"items:\", dataloader.n_items, \"user:\", dataloader.n_users)\n",
    "model = LightGCNCustom(data=dataloader, hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()\n",
    "model.run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.recommend_k_items(\n",
    "    dataloader.test, \n",
    "    top_k=3, \n",
    "    use_id=True, \n",
    "    remove_seen=True, \n",
    "    recommend_from=folds[-1][3]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k, r_precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLightGCN(tune.Trainable):\n",
    "    def setup(\n",
    "        self,\n",
    "        config: Dict[str, Any],\n",
    "        data,\n",
    "    ):\n",
    "        self.config = config\n",
    "\n",
    "        self.hparams = prepare_hparams(\n",
    "            model_type='lightgcn',\n",
    "            n_layers=config['conv_layers'],\n",
    "            batch_size=2**config['batch_size'],\n",
    "            embed_size=config['embedding_dim'],\n",
    "            epochs=EPOCHS_PER_ITER,\n",
    "            learning_rate=config['learning_rate'],\n",
    "            decay=config['l2'],\n",
    "            metrics=METRICS,\n",
    "            eval_epoch=-1,\n",
    "            top_k=K_RECOMMENDATIONS[0],\n",
    "            save_model=False,\n",
    "            MODEL_DIR='./data/model/lightgcn/',\n",
    "        )\n",
    "\n",
    "        train, test, self.t, self.open_proposals = data\n",
    "        train_filtered = filter_window_size(train, self.t, config['window_size'])\n",
    "        self.dataloader = ImplicitCF(train=train_filtered, test=test, seed=SEED)\n",
    "        self.model = LightGCNCustom(self.hparams, self.dataloader, seed=SEED)\n",
    "        self.total_train = 0\n",
    "        self.total_eval = 0\n",
    "\n",
    "    @property\n",
    "    def iteration(self):\n",
    "        return self.model.epochs_done\n",
    "\n",
    "    @property\n",
    "    def training_iteration(self):\n",
    "        return self.model.epochs_done\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        As a rule of thumb, the execution time of step should be large enough to avoid overheads \n",
    "        (i.e. more than a few seconds), but short enough to report progress periodically \n",
    "        (i.e. at most a few minutes).\n",
    "        \"\"\"\n",
    "        assert EPOCHS_PER_ITER > 0\n",
    "\n",
    "        train_start = time.time()\n",
    "        for _ in range(EPOCHS_PER_ITER):\n",
    "            ret = self.model.fit_epoch()\n",
    "        eval_start = train_end = time.time()\n",
    "\n",
    "        eval_dict = {'model_'+k:v for k,v in zip(self.model.metrics, self.model.run_eval())}\n",
    "        for k in K_RECOMMENDATIONS:\n",
    "            recs = self.model.recommend_k_items(\n",
    "                self.dataloader.test, \n",
    "                top_k=k,\n",
    "                use_id=True, \n",
    "                remove_seen=True, \n",
    "                recommend_from=self.open_proposals,\n",
    "            )\n",
    "            \n",
    "            eval_dict[f'precision@{k}'] = precision_at_k(self.dataloader.test, recs, k=k)\n",
    "            eval_dict[f'ndcg@{k}'] = ndcg_at_k(self.dataloader.test, recs, k=k)\n",
    "            eval_dict[f'recall@{k}'] = recall_at_k(self.dataloader.test, recs, k=k)\n",
    "            eval_dict[f'map@{k}'] = map_at_k(self.dataloader.test, recs, k=k)\n",
    "            eval_dict[f'rprecision@{k}'] = r_precision_at_k(self.dataloader.test, recs, k=k)\n",
    "\n",
    "        eval_end = time.time()\n",
    "\n",
    "        self.total_train += train_end - train_start\n",
    "        self.total_eval += eval_end - eval_start\n",
    "        \n",
    "        return {\n",
    "            'iteration': self.iteration,\n",
    "            'loss': ret[0],\n",
    "            'mf_loss': ret[1],\n",
    "            'emb_loss': ret[2],\n",
    "            **eval_dict,\n",
    "            'time_train': train_end-train_start,\n",
    "            'time_test': eval_end-eval_start,\n",
    "            'time_total_train': self.total_train,\n",
    "            'time_total_test': self.total_eval,\n",
    "        }\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "        self.model.saver.save(\n",
    "            sess=self.model.sess,\n",
    "            save_path=checkpoint_path,\n",
    "        )\n",
    "        return checkpoint_dir\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        self.model.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAY_RESULTS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.uname().nodename)\n",
    "\n",
    "### SET TRAINING RESOURCES\n",
    "if os.uname().nodename == 'lamarck':\n",
    "    # assert torch.cuda.is_available()\n",
    "\n",
    "    NUM_SAMPLES = SAMPLES_PER_SPLIT\n",
    "    # Every run takes approx half a gig of vram (no optimizations)\n",
    "    # The RTX 4090 has 24GB so we can run the model about 48 times\n",
    "    resources_per_trial={\n",
    "        'cpu': 1,\n",
    "        'gpu': 1 / GPUS,\n",
    "    }\n",
    "else:\n",
    "    NUM_SAMPLES = 1\n",
    "    resources_per_trial={\n",
    "        'cpu': 1,\n",
    "        # It takes about 1.5 GiB with full training data, but I put a bit more because\n",
    "        # this notebook also takes a bit of memory\n",
    "        'memory': 2e9,\n",
    "    }\n",
    "print(resources_per_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.search.hyperopt import HyperOptSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTunerOnFold(f, points_to_evaluate = None):    \n",
    "    name = paths.lightgcn_ray_tune_fname(ORG_NAME, SPLITS_FREQ, SPLITS_NORMALIZE, OPTIM_METRIC, fold=f)\n",
    "    experiments = list(RAY_RESULTS_PATH.glob(f'{name}_*'))\n",
    "    last_experiment = max(experiments, key=lambda x: x.stat().st_ctime) if experiments else None\n",
    "\n",
    "    dftrain,dftest,t,open_proposals = folds[f]\n",
    "    param_space = dict(\n",
    "        fold=f,\n",
    "        batch_size=tune.randint(6, MAX_BATCH_SIZE), # 64 - 1024\n",
    "        embedding_dim=tune.lograndint(1, MAX_EMBEDDING_DIM, base=2),\n",
    "        conv_layers=tune.randint(1,4),\n",
    "        learning_rate=tune.qloguniform(MIN_LR, 1, 1e-4),\n",
    "        l2=tune.loguniform(1e-7, 1e-2, 1e-7),\n",
    "        window_size=tune.choice(WINDOW_SIZES),\n",
    "    )\n",
    "    \n",
    "    ### RESTORE EXPERIMENT OR CREATE A NEW ONE\n",
    "    if last_experiment and tune.Tuner.can_restore(last_experiment):\n",
    "        print(f\"Restoring last experiment: {last_experiment}\")\n",
    "        tuner = tune.Tuner.restore(\n",
    "            str(last_experiment),\n",
    "            trainable=tune.with_resources(\n",
    "                # tune.with_parameters(TrainLightGCN,  train=dftrain, test=dftest, open_proposals=open_proposals),\n",
    "                tune.with_parameters(TrainLightGCN, data=folds[f]),\n",
    "                resources_per_trial,\n",
    "            ),\n",
    "            restart_errored=True,\n",
    "            param_space=param_space,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"No experiment found for fold {f}, creating new tuner with {NUM_SAMPLES} samples\")\n",
    "        search_alg = None\n",
    "        search_alg = HyperOptSearch(\n",
    "            # points_to_evaluate=[{\n",
    "            #     'batch_size': 8, # 2**8 = 256\n",
    "            #     'learning_rate': 10e-2,\n",
    "            #     'l2': 10e-6,\n",
    "            #     'embedding_dim': 100,\n",
    "            #     'conv_layers': 3,\n",
    "            # }],\n",
    "            points_to_evaluate = points_to_evaluate,\n",
    "            random_state_seed=SEED,\n",
    "        )\n",
    "        # search_alg = tune.search.Repeater(search_alg, N_SPLITS-SKIP_SPLIT)\n",
    "        \n",
    "        tuner = tune.Tuner(\n",
    "            tune.with_resources(\n",
    "                # tune.with_parameters(TrainLightGCN,  train=dftrain, test=dftest, open_proposals=open_proposals),\n",
    "                tune.with_parameters(TrainLightGCN, data=folds[f]),\n",
    "                resources_per_trial,\n",
    "            ),\n",
    "            run_config=train.RunConfig(\n",
    "                stop={'training_iteration': MAX_EPOCHS/EPOCHS_PER_ITER, 'time_total_train': 300},\n",
    "                name=name + f'_{dt.datetime.now().isoformat()}',\n",
    "                storage_path=RAY_RESULTS_PATH,\n",
    "                # failure_config=train.FailureConfig(fail_fast='raise'),\n",
    "                failure_config=train.FailureConfig(max_failures=3),\n",
    "            ),\n",
    "            param_space=param_space,\n",
    "            tune_config=tune.TuneConfig(\n",
    "                search_alg=search_alg,\n",
    "                num_samples=NUM_SAMPLES,\n",
    "                metric=OPTIM_METRIC,\n",
    "                mode='max',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def findConfig(rg):\n",
    "    for r in rg:\n",
    "        if r.config:\n",
    "            if all((r.config[k] == v for k, v in last_best_result.config.items() if k != 'fold')):\n",
    "                return r\n",
    "\n",
    "    return None\n",
    "\n",
    "tuners = []\n",
    "results = []\n",
    "last_best_result = None\n",
    "for f in range(LAST_SPLITS):\n",
    "    best_prev_config = None\n",
    "    if last_best_result is not None:\n",
    "        best_prev_config = last_best_result.config.copy()\n",
    "        best_prev_config['fold'] += 1\n",
    "        best_prev_config = [best_prev_config]\n",
    "    \n",
    "    t = getTunerOnFold(f, best_prev_config)\n",
    "    tuners.append(t)\n",
    "\n",
    "    rg = t.fit()\n",
    "    assert rg.num_errors == 0, f\"There are {rg.num_errors} errors\"\n",
    "    assert rg.num_terminated >= NUM_SAMPLES, f'Some samples are not terminated ({rg.num_terminated} != {NUM_SAMPLES})'\n",
    "    results.append(rg)\n",
    "\n",
    "    # Assert that the prev config has been tried\n",
    "    if last_best_result is not None:\n",
    "        # if not any( \n",
    "        #     all((r.config[k] == v for k, v in last_best_result.config.items() if k != 'fold'))\n",
    "        #     for r in rg if r.config\n",
    "        # ):\n",
    "        if not findConfig(rg):\n",
    "            print(\"Best config:\", last_best_result.config)\n",
    "            assert False, f\"The best config from previous fold has not been tested in fold {f}\"    \n",
    "        else:\n",
    "            logging.info(f'Fold {f}. Best prev result was {last_best_result.path} and config has been found {findConfig(rg).path}')\n",
    "    \n",
    "    last_best_result = rg.get_best_result()\n",
    "\n",
    "    print(f\"Finished training for fold {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
